{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save images as HOG features\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def extract_hog_features(image_folder, output_csv, \n",
    "                         orientations=9, pixels_per_cell=(8, 8), \n",
    "                         cells_per_block=(2, 2), filter_keyword=\"_grayscale\"):\n",
    "    features = []\n",
    "    image_names = []\n",
    "    \n",
    "    for image_name in os.listdir(image_folder):\n",
    "        if filter_keyword in image_name:\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            try:\n",
    "                image = imread(image_path)\n",
    "                \n",
    "                if len(image.shape) == 3:\n",
    "                    image = rgb2gray(image)\n",
    "                    \n",
    "                hog_features = hog(image, \n",
    "                                   orientations=orientations, \n",
    "                                   pixels_per_cell=pixels_per_cell, \n",
    "                                   cells_per_block=cells_per_block, \n",
    "                                   block_norm='L2-Hys', \n",
    "                                   visualize=False)\n",
    "                \n",
    "                features.append(hog_features)\n",
    "                image_names.append(image_name)\n",
    "                print(f\"Processed: {image_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{image_name}': {e}\")\n",
    "\n",
    "    df = pd.DataFrame(features)\n",
    "    df.insert(0, 'image_name', image_names)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"HOG features saved to {output_csv}\")\n",
    "\n",
    "input_folder = 'train_new_ims' \n",
    "output_file = 'hog_features_grayscale.csv' \n",
    "\n",
    "extract_hog_features(image_folder=input_folder, output_csv=output_file, filter_keyword=\"_grayscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and save histogram features\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_histogram_features(image_folder, output_csv, bins=8, exclude_keyword=\"_grayscale\"):\n",
    "    histogram_features = []\n",
    "    image_names = []\n",
    "\n",
    "    for image_name in os.listdir(image_folder):\n",
    "        # Process files that do NOT contain \"grey_scale\"\n",
    "        if exclude_keyword not in image_name:\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            try:\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                if image is None:\n",
    "                    raise ValueError(\"Image not loaded correctly.\")\n",
    "\n",
    "                # Convert the image to HSV color space, we can try RGB as well\n",
    "                hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "                # Compute the color histogram for H, S, and V channels\n",
    "                hist_h = cv2.calcHist([hsv_image], [0], None, [bins], [0, 180])\n",
    "                hist_s = cv2.calcHist([hsv_image], [1], None, [bins], [0, 256])\n",
    "                hist_v = cv2.calcHist([hsv_image], [2], None, [bins], [0, 256])\n",
    "\n",
    "                # Normalize the histograms\n",
    "                hist_h = hist_h / hist_h.sum()\n",
    "                hist_s = hist_s / hist_s.sum()\n",
    "                hist_v = hist_v / hist_v.sum()\n",
    "\n",
    "                hist_features = np.concatenate((hist_h.flatten(), hist_s.flatten(), hist_v.flatten()))\n",
    "\n",
    "                histogram_features.append(hist_features)\n",
    "                image_names.append(image_name)\n",
    "                print(f\"Processed: {image_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_name}: {e}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    hist_df = pd.DataFrame(histogram_features)\n",
    "    hist_df.insert(0, 'image_name', image_names)\n",
    "    hist_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Histogram features saved to {output_csv}\")\n",
    "\n",
    "input_folder = 'train_new_ims' \n",
    "output_file = 'histogram_features_color.csv' \n",
    "\n",
    "extract_histogram_features(image_folder=input_folder, output_csv=output_file, bins=8, exclude_keyword=\"_grayscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_hog_and_histogram_with_hist_image_name(hog_csv, hist_csv, output_csv):\n",
    "    hog_df = pd.read_csv(hog_csv)\n",
    "    hist_df = pd.read_csv(hist_csv)\n",
    "\n",
    "    hog_df['image_code'] = hog_df['image_name'].str.split('_').str[0] #get the first part of their name\n",
    "    hist_df['image_code'] = hist_df['image_name'].str.split('_').str[0] #get the first part of their name\n",
    "\n",
    "    combined_df = pd.merge(hog_df, hist_df, on='image_code', how='inner', suffixes=('_hog', '_hist'))\n",
    "\n",
    "    combined_df['image_name'] = combined_df['image_name_hist'] #use the histogram image name\n",
    "    combined_df = combined_df.drop(columns=['image_name_hog', 'image_name_hist'])\n",
    "\n",
    "    cols = ['image_name'] + [col for col in combined_df.columns if col != 'image_name']\n",
    "    combined_df = combined_df[cols]\n",
    "\n",
    "    # Save the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Combined features saved to {output_csv}\")\n",
    "    return combined_df\n",
    "\n",
    "hog_csv = 'hog_features_grayscale.csv'\n",
    "hist_csv = 'histogram_features_color.csv'\n",
    "output_csv = 'combined_features.csv'\n",
    "\n",
    "combined_features_df = combine_hog_and_histogram_with_hist_image_name(hog_csv, hist_csv, output_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_name', '0_hog', '1_hog', '2_hog', '3_hog', '4_hog', '5_hog',\n",
      "       '6_hog', '7_hog', '8_hog',\n",
      "       ...\n",
      "       '14_hist', '15_hist', '16_hist', '17_hist', '18_hist', '19_hist',\n",
      "       '20_hist', '21_hist', '22_hist', '23_hist'],\n",
      "      dtype='object', length=350)\n",
      "              image_name     0_hog     1_hog     2_hog     3_hog     4_hog  \\\n",
      "0  9e1d819_augmented.jpg  0.139699  0.079391  0.038862  0.090377  0.253900   \n",
      "1  9e1d819_augmented.jpg  0.165667  0.255557  0.058470  0.104116  0.095903   \n",
      "\n",
      "      5_hog     6_hog     7_hog     8_hog  ...   14_hist   15_hist   16_hist  \\\n",
      "0  0.217409  0.253900  0.069572  0.083052  ...  0.000977  0.000977  0.181641   \n",
      "1  0.039462  0.015208  0.037321  0.027881  ...  0.000977  0.000977  0.181641   \n",
      "\n",
      "    17_hist   18_hist   19_hist   20_hist   21_hist   22_hist   23_hist  \n",
      "0  0.112305  0.143555  0.182617  0.166992  0.108398  0.083984  0.020508  \n",
      "1  0.112305  0.143555  0.182617  0.166992  0.108398  0.083984  0.020508  \n",
      "\n",
      "[2 rows x 350 columns]\n",
      "_____________\n",
      "Index(['image_name', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
      "       ...\n",
      "       '314', '315', '316', '317', '318', '319', '320', '321', '322', '323'],\n",
      "      dtype='object', length=325)\n",
      "                        image_name         0         1         2         3  \\\n",
      "0  9e1d819_augmented_grayscale.jpg  0.139699  0.079391  0.038862  0.090377   \n",
      "1  7db1b9e_augmented_grayscale.jpg  0.385494  0.109185  0.097542  0.120223   \n",
      "2  b877c16_augmented_grayscale.jpg  0.190035  0.083034  0.077333  0.193428   \n",
      "3            3e67b7f_grayscale.jpg  0.074741  0.121858  0.236754  0.023257   \n",
      "4  0abffee_augmented_grayscale.jpg  0.068410  0.019385  0.030614  0.057324   \n",
      "\n",
      "          4         5         6         7         8  ...       314       315  \\\n",
      "0  0.253900  0.217409  0.253900  0.069572  0.083052  ...  0.000000  0.213834   \n",
      "1  0.385494  0.003338  0.000000  0.000926  0.001707  ...  0.009461  0.312023   \n",
      "2  0.255233  0.255233  0.103145  0.075526  0.027094  ...  0.044976  0.132882   \n",
      "3  0.035792  0.022944  0.030104  0.014575  0.009551  ...  0.206541  0.058513   \n",
      "4  0.334587  0.205220  0.004417  0.048003  0.018371  ...  0.119643  0.185413   \n",
      "\n",
      "        316       317       318       319       320       321       322  \\\n",
      "0  0.034591  0.042369  0.038867  0.376280  0.136476  0.020202  0.001379   \n",
      "1  0.075190  0.000000  0.098225  0.159547  0.122230  0.059480  0.049860   \n",
      "2  0.049192  0.141096  0.170136  0.141794  0.023513  0.025421  0.034670   \n",
      "3  0.309596  0.309596  0.134744  0.040588  0.007578  0.000000  0.038224   \n",
      "4  0.045563  0.014009  0.141704  0.086891  0.101026  0.061804  0.045625   \n",
      "\n",
      "        323  \n",
      "0  0.046369  \n",
      "1  0.312023  \n",
      "2  0.024422  \n",
      "3  0.004463  \n",
      "4  0.214858  \n",
      "\n",
      "[5 rows x 325 columns]\n",
      "_____________\n",
      "Index(['image_name', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
      "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
      "       '23'],\n",
      "      dtype='object')\n",
      "              image_name         0         1         2         3         4  \\\n",
      "0            6c3a28b.jpg  0.339844  0.185547  0.015625  0.025391  0.348633   \n",
      "1  4b01403_augmented.jpg  0.403320  0.014648  0.003906  0.037109  0.227539   \n",
      "2            120d8a4.jpg  0.354492  0.126953  0.181641  0.083984  0.040039   \n",
      "3  a3ef4f8_augmented.jpg  0.011719  0.055664  0.232422  0.188477  0.390625   \n",
      "4            72a2afe.jpg  0.441406  0.333984  0.077148  0.024414  0.019531   \n",
      "\n",
      "          5         6         7         8  ...        14        15        16  \\\n",
      "0  0.059570  0.002930  0.022461  0.514648  ...  0.003906  0.007812  0.044922   \n",
      "1  0.059570  0.071289  0.182617  0.733398  ...  0.000977  0.000977  0.026367   \n",
      "2  0.030273  0.031250  0.151367  0.185547  ...  0.010742  0.009766  0.433594   \n",
      "3  0.087891  0.011719  0.021484  0.219727  ...  0.037109  0.246094  0.418945   \n",
      "4  0.021484  0.022461  0.059570  0.405273  ...  0.012695  0.002930  0.130859   \n",
      "\n",
      "         17        18        19        20        21        22        23  \n",
      "0  0.191406  0.203125  0.165039  0.126953  0.075195  0.078125  0.115234  \n",
      "1  0.017578  0.055664  0.101562  0.125977  0.187500  0.151367  0.333984  \n",
      "2  0.102539  0.099609  0.216797  0.103516  0.008789  0.026367  0.008789  \n",
      "3  0.162109  0.138672  0.122070  0.085938  0.054688  0.013672  0.003906  \n",
      "4  0.238281  0.263672  0.113281  0.041992  0.053711  0.092773  0.065430  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.read_csv('combined_features.csv')\n",
    "print(combined_df.columns)\n",
    "print(combined_df.head(2))\n",
    "\n",
    "hog_csv = pd.read_csv('hog_features_grayscale.csv')\n",
    "hist_csv = pd.read_csv('histogram_features_color.csv')\n",
    "print(\"_____________\")\n",
    "print(hog_csv.columns)\n",
    "print(hog_csv.head(5))\n",
    "print(\"_____________\")\n",
    "print(hist_csv.columns)\n",
    "print(hist_csv.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv(\"hog_features_grayscale.csv\").isna().sum().sum())\n",
    "print(pd.read_csv(\"histogram_features_color.csv\").isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Image Codes:\n",
      "0    9e1d819\n",
      "1    9e1d819\n",
      "2    7db1b9e\n",
      "3    7db1b9e\n",
      "4    b877c16\n",
      "Name: image_code, dtype: object\n",
      "\n",
      "Training Labels Image Codes:\n",
      "0    00016cd\n",
      "1    0001808\n",
      "2    0002399\n",
      "3    0003973\n",
      "4    00061cc\n",
      "Name: image_code, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Combined features\n",
    "combined_df = pd.read_csv(\"combined_features.csv\")\n",
    "combined_df['image_code'] = combined_df['image_name'].str.split('_').str[0]\n",
    "print(\"Combined Features Image Codes:\")\n",
    "print(combined_df['image_code'].head())\n",
    "\n",
    "# Training labels\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df['image_code'] = train_df['im_name'].str.split('.').str[0]\n",
    "print(\"\\nTraining Labels Image Codes:\")\n",
    "print(train_df['image_code'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 100000 features with labels.\n"
     ]
    }
   ],
   "source": [
    "def load_combined_features_for_training(combined_csv, train_csv):\n",
    "\n",
    "    combined_df = pd.read_csv(combined_csv)\n",
    "    # Load the labels\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "\n",
    "    # Extract first part of image name for matching\n",
    "    combined_df['image_code'] = combined_df['image_name'].str.split('_').str[0]\n",
    "    train_df['image_code'] = train_df['im_name'].str.split('.').str[0]\n",
    "\n",
    "    # Merge combined features with labels\n",
    "    matched_df = pd.merge(combined_df, train_df, on='image_code', how='inner')\n",
    "\n",
    "    X = matched_df.drop(columns=['image_name', 'image_code', 'im_name', 'label']).values\n",
    "    y = matched_df['label'].values\n",
    "\n",
    "    print(f\"Matched {len(matched_df)} features with labels.\")\n",
    "    return X, y\n",
    "\n",
    "X, y = load_combined_features_for_training(\"combined_features.csv\", \"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 348) (100000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 348) (20000, 348) (80000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', max_iter=10000, tol=1e-3, random_state=42)\n",
    "#log_loss = logistic regression, hinge = linear SVM, \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4099\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4137\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4692\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4688\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4322\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4226\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4755\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4745\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.3849\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.3784\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4203\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4412\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4359\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4244\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4773\n",
      "Testing parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4770\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.3947\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.3690\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4548\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4501\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4288\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.3823\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4650\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4613\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.3486\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.3367\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4007\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4137\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4290\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.3883\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Accuracy: 0.4649\n",
      "Testing parameters: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'elasticnet'}\n",
      "Accuracy: 0.4617\n",
      "\n",
      "Best Parameters: {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log_loss', 'max_iter': 500, 'penalty': 'l2'}\n",
      "Best Test Accuracy: 0.4773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'log_loss'],  \n",
    "    'penalty': ['l2', 'elasticnet'],  \n",
    "    'alpha': [1e-4, 1e-3],  \n",
    "    'learning_rate': ['constant', 'adaptive'],  \n",
    "    'eta0': [0.01, 0.1], \n",
    "    'max_iter': [500],  \n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for params in grid:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    clf = SGDClassifier(random_state=42, **params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)  \n",
    "    acc = accuracy_score(y_test, y_pred)  \n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
