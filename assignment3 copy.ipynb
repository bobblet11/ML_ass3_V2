{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel' ; got (vote=%r)\" % self.vote)\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers' % (len(self.weights), len(self.classifiers)))\n",
    "    \n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            binary_labels = (y == clf['label']).astype(int)\n",
    "            fitted_clf = clone(clf['model']).fit(X, binary_labels)\n",
    "            self.classifiers_.append({'model':fitted_clf, 'label': clf['label']})\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # print(X[0])\n",
    "        # Using decision function for class predictions\n",
    "        predictions = np.asarray([clf['model'].predict(X) for clf in self.classifiers_]).T\n",
    "        maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions)\n",
    "        print(predictions)\n",
    "        print(maj_vote)\n",
    "        return maj_vote\n",
    "        print(\"Sample Predictions:\", predictions)\n",
    "\n",
    "        # Perform majority voting\n",
    "        maj_vote.append(np.bincount(predictions).argmax())\n",
    "        return maj_vote\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        y_predict = self.predict(X_test)\n",
    "        return accuracy_score(Y_test, y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FETCHING IMAGES FROM DIRECTORY\n",
      "\n",
      "Progress: 100.00%100000\n",
      "TOTAL SAMPLES: 100000\n",
      "OUTER SPLIT: training[43750]\ttesting[6250]\ttotal[50000]\n",
      "\tINNER SPLIT: training[32812]\tvalidation[10938]\ttotal[43750]\n",
      "65624\n",
      "65624\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy.core.defchararray as np_f\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import sys\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "def mix_aug_data(X_train, Y_train, AUG_NAME_MODIFIER):\n",
    "    # print(\"\\nMIXING AUGMENT\\n\")\n",
    "    image_paths = X_train.flatten() \n",
    "    augmented_image_paths =    augmented_image_paths = np.array([path.replace('.jpg', AUG_NAME_MODIFIER) for path in image_paths])\n",
    "    augmented_image_paths = augmented_image_paths.reshape(X_train.shape)\n",
    "    result_X_train = np.concatenate((augmented_image_paths, X_train), axis=0)\n",
    "    result_Y_train = np.concatenate((Y_train, Y_train), axis=0)\n",
    "    return result_X_train, result_Y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_train, Y_train, X_eval, Y_eval, model):\n",
    "    \"\"\"Train the KNN model and evaluate it on the validation set.\"\"\"\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred_validation = model.predict(X_eval)\n",
    "    return accuracy_score(Y_eval, y_pred_validation)\n",
    "\n",
    "def gridSearch(X_train, Y_train, X_eval, Y_eval):\n",
    "    #grid search\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    # print(Y_eval)\n",
    "    # print(binary_labels_eval)\n",
    "\n",
    "\n",
    "    # Iterate over each parameter combination\n",
    "    for params in param_combinations:\n",
    "        # Create a new KNN model with the current parameters\n",
    "        model = OneVsRestClassifier(SVC(**params))\n",
    "        # Train the model\n",
    "        model.fit(X_train, Y_train)\n",
    "        # Validate the model\n",
    "        y_pred_eval = model.predict(X_eval)\n",
    "    \n",
    "        validation_accuracy = accuracy_score(Y_eval, y_pred_eval)\n",
    "\n",
    "        # Check if this is the best score\n",
    "        if validation_accuracy > best_score:\n",
    "            best_score = validation_accuracy\n",
    "            best_params = params\n",
    "\n",
    "    return best_score, best_params\n",
    "\n",
    "\n",
    "def get_data_dict(X):\n",
    "\n",
    "    print(\"\\nFETCHING IMAGES FROM DIRECTORY\\n\")\n",
    "\n",
    "    image_paths = [f\"train_new_ims/{img[0]}\" for img in X]\n",
    "    img_dict = {}\n",
    "    total_images = len(image_paths)\n",
    "\n",
    "    def progress_callback(future):\n",
    "        \"\"\"Callback function to update progress.\"\"\"\n",
    "        nonlocal loaded_images\n",
    "        loaded_images += 1\n",
    "        percentage = (loaded_images / total_images) * 100\n",
    "        print('\\r\\033[K', end='')\n",
    "        print(f\"\\rProgress: {percentage:.2f}%\", end='')\n",
    "\n",
    "    loaded_images = 0\n",
    "\n",
    "    # Use ThreadPoolExecutor to load images in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to load images\n",
    "        futures = {executor.submit(load_image, path): path for path in image_paths}\n",
    "        \n",
    "        # Attach a callback to each future to update progress\n",
    "        for future in futures:\n",
    "            future.add_done_callback(progress_callback)\n",
    "\n",
    "        # Collect results\n",
    "        for future in as_completed(futures):\n",
    "            img_path = futures[future]\n",
    "            img_dict[img_path.split('/')[-1]] = future.result()  # Store the result in the dictionary\n",
    "        \n",
    "    print(len(list(img_dict.keys())))\n",
    "    return img_dict\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img_ = tf.keras.preprocessing.image.load_img(path)  # Load image\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img_)  # Convert to array\n",
    "    mean = np.mean(img_array)\n",
    "    std_dev = np.std(img_array)\n",
    "    img_array = (img_array - mean) / std_dev\n",
    "    return img_array.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_dict(X_image_paths, all_images):\n",
    "    # print(\"\\nFETCHING IMAGES FROM DICTIONARY\\n\")\n",
    "    # Extract the image keys (filenames) from X_image_paths\n",
    "    img_keys = [img[0] for img in X_image_paths]\n",
    "    # Fetch the corresponding pixel data from the dictionary\n",
    "    img_arrays = [all_images[key] for key in img_keys if key in all_images]\n",
    "    return np.array(img_arrays)\n",
    "\n",
    "\n",
    "def load_combined_features_for_training(combined_csv, train_csv):\n",
    "    combined_df = pd.read_csv(combined_csv)\n",
    "    # Load the labels\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    # Extract first part of image name for matching\n",
    "    combined_df['image_code'] = combined_df['image_name'].str.split('_').str[0]\n",
    "    train_df['image_code'] = train_df['im_name'].str.split('.').str[0]\n",
    "    # Merge combined features with labels\n",
    "    matched_df = pd.merge(combined_df, train_df, on='image_code', how='inner')\n",
    "    X = matched_df.drop(columns=['image_name', 'image_code', 'im_name', 'label']).values\n",
    "    y = matched_df['label'].values\n",
    "    print(f\"Matched {len(matched_df)} features with labels.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "xs = np.array(df.iloc[:, :-1])\n",
    "ys = np.array(df.iloc[:, -1])\n",
    "outer_cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=22)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 0.2, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['rbf'],   # SVM kernel types\n",
    "    'gamma': ['auto']      # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Create a list of parameter combinations\n",
    "param_combinations = [\n",
    "    \n",
    "    {'C': c, 'kernel': k, 'gamma': g}\n",
    "    for c in param_grid['C']\n",
    "    for k in param_grid['kernel']\n",
    "    for g in param_grid['gamma']\n",
    "]\n",
    "\n",
    "# BAYESIAN\n",
    "# param_space = {\n",
    "#     'n_neighbors': (450, 500),  # Range of neighbors\n",
    "#     'metric': ['minkowski'],\n",
    "#     'p': (2, 5)  # 1 for Manhattan, 2 for Euclidean\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########LOAD ALL IMAGES INTO MEMORY#############\n",
    "#load all images into a dictionary for future access. Should be faster since images are loaded only once into memory.\n",
    "\n",
    "X_HIST, Y_HIST = load_combined_features_for_training(\"combined_features.csv\", \"train.csv\")\n",
    "\n",
    "all_image_paths, throwAways = mix_aug_data(xs,ys, \"_augmented.jpg\")\n",
    "all_images = get_data_dict(all_image_paths)\n",
    "\n",
    "print(f\"TOTAL SAMPLES: {len(all_image_paths)}\")\n",
    "total_accuracy = 0\n",
    "number_of_test_rounds = 0\n",
    "######### NESTED K FOLD ##############\n",
    "######### TRAINING + TEST FOLDS ######\n",
    "for i, (train_index, test_index) in enumerate(outer_cv.split(xs,ys)):\n",
    "    #not pixels at this point, just image names.\n",
    "    X_train, X_test = xs[train_index], xs[test_index]\n",
    "    Y_train, Y_test = ys[train_index], ys[test_index]\n",
    "    total_validation_accuracy = 0\n",
    "    number_of_validation_rounds = 0\n",
    "\n",
    "    print(f\"OUTER SPLIT: training[{len(X_train)}]\\ttesting[{len(X_test)}]\\ttotal[{len(X_train) + len(X_test)}]\")\n",
    "\n",
    "    best_across_validation = {'best_score':None, 'best_params':None}\n",
    "\n",
    "    ######### TRAINING + VALIDATION FOLDS ######\n",
    "    for j, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train,Y_train)):\n",
    "\n",
    "        X_inner_train, X_inner_val = X_train[inner_train_index], X_train[inner_val_index]\n",
    "        y_inner_train, y_inner_val = Y_train[inner_train_index], Y_train[inner_val_index]\n",
    "        print(f\"\\tINNER SPLIT: training[{len(X_inner_train)}]\\tvalidation[{len(X_inner_val)}]\\ttotal[{len(X_inner_train) + len(X_inner_val)}]\")\n",
    "        X_inner_train, y_inner_train = mix_aug_data(X_inner_train, y_inner_train, \"_augmented.jpg\")\n",
    "        X_inner_train = get_data_from_dict(X_inner_train, all_images)\n",
    "        X_inner_val = get_data_from_dict(X_inner_val, all_images)\n",
    "\n",
    "        best_score, best_params = gridSearch(X_inner_train, y_inner_train, X_inner_val, y_inner_val)\n",
    "        print(f\"\\tOuter Fold {i + 1}, Inner Fold {j + 1}, Best Accuracy: {best_score:.4f}\")\n",
    "        if (best_across_validation['best_score'] == None or best_across_validation['best_score'] < best_score):\n",
    "            best_across_validation['best_score'] = best_score\n",
    "            best_across_validation['best_params'] = best_params\n",
    "\n",
    "        total_validation_accuracy += best_score\n",
    "        number_of_validation_rounds += 1\n",
    "    \n",
    "    average_validation_acc = total_validation_accuracy/number_of_validation_rounds\n",
    "    print(f\"Average Validation Accuracy: {average_validation_acc:.4f}\")\n",
    "\n",
    "    X_train, Y_train = mix_aug_data(X_train, Y_train, \"_augmented.jpg\")\n",
    "    X_train = get_data_from_dict(X_train, all_images)\n",
    "    X_test = get_data_from_dict(X_test,all_images)\n",
    "    best_model = OneVsRestClassifier(SVC(**best_across_validation['best_params']))\n",
    "    best_model.fit(X_inner_train,y_inner_train)\n",
    "\n",
    "    test_accuracy = train_and_evaluate(X_train, Y_train, X_test, Y_test, best_model)\n",
    "    print(f\"Outer Fold {i+1} Accuracy: {test_accuracy:.4f}\\n\\n\")\n",
    "    total_accuracy += test_accuracy\n",
    "    number_of_test_rounds +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "average_test_acc = total_accuracy/number_of_test_rounds\n",
    "print(f\"Average Test Accuracy: {average_test_acc:.4f}\") \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
