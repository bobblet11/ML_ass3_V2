{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel' ; got (vote=%r)\" % self.vote)\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers' % (len(self.weights), len(self.classifiers)))\n",
    "    \n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            binary_labels = (y == clf['label']).astype(int)\n",
    "            fitted_clf = clone(clf['model']).fit(X, binary_labels)\n",
    "            self.classifiers_.append({'model':fitted_clf, 'label': clf['label']})\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # print(X[0])\n",
    "        # Using decision function for class predictions\n",
    "        predictions = np.asarray([clf['model'].predict(X) for clf in self.classifiers_]).T\n",
    "        maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions)\n",
    "        print(predictions)\n",
    "        print(maj_vote)\n",
    "        return maj_vote\n",
    "        print(\"Sample Predictions:\", predictions)\n",
    "\n",
    "        # Perform majority voting\n",
    "        maj_vote.append(np.bincount(predictions).argmax())\n",
    "        return maj_vote\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        y_predict = self.predict(X_test)\n",
    "        return accuracy_score(Y_test, y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 385\u001b[0m\n\u001b[0;32m    378\u001b[0m     average_test_acc \u001b[38;5;241m=\u001b[39m total_accuracy\u001b[38;5;241m/\u001b[39mnumber_of_test_rounds\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENSEMBLE Average Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m--> 385\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m xs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    387\u001b[0m ys \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy.core.defchararray as np_f\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import sys\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def mix_aug_data(X_train, Y_train, AUG_NAME_MODIFIER):\n",
    "    # print(\"\\nMIXING AUGMENT\\n\")\n",
    "    image_paths = X_train.flatten() \n",
    "    augmented_image_paths =    augmented_image_paths = np.array([path.replace('.jpg', AUG_NAME_MODIFIER) for path in image_paths])\n",
    "    augmented_image_paths = augmented_image_paths.reshape(X_train.shape)\n",
    "    result_X_train = np.concatenate((augmented_image_paths, X_train), axis=0)\n",
    "    result_Y_train = np.concatenate((Y_train, Y_train), axis=0)\n",
    "    return result_X_train, result_Y_train\n",
    "\n",
    "\n",
    "def get_gray_scales(X_train, AUG_NAME_MODIFIER='_grayscale.jpg'):\n",
    "    # print(\"\\nMIXING AUGMENT\\n\")\n",
    "    image_paths = X_train.flatten() \n",
    "    augmented_image_paths =    augmented_image_paths = np.array([path.replace('.jpg', AUG_NAME_MODIFIER) for path in image_paths])\n",
    "    augmented_image_paths = augmented_image_paths.reshape(X_train.shape)\n",
    "    return augmented_image_paths\n",
    "\n",
    "def load_dict_HOG():\n",
    "    df = pd.read_csv('./hog_features_grayscale.csv')\n",
    "    xs = np.array(df.iloc[:, :1])\n",
    "    ys = np.array(df.iloc[:, 1:])\n",
    "    return make_HOG_dict(xs,ys)\n",
    "\n",
    "def load_dict_HIST():\n",
    "    df = pd.read_csv('./histogram_features_color.csv')\n",
    "    xs = np.array(df.iloc[:, :1])\n",
    "    ys = np.array(df.iloc[:, 1:])\n",
    "    return make_HIST_dict(xs,ys)\n",
    "\n",
    "def make_HOG_dict(xs,ys):\n",
    "    HOG = {}\n",
    "    for i in range(len(xs)):\n",
    "        HOG[xs[i][0]] = ys[i]\n",
    "    print(\"loaded HOG\")\n",
    "    return HOG\n",
    "\n",
    "\n",
    "def make_HIST_dict(xs,ys):\n",
    "    HIST = {}\n",
    "    for i in range(len(xs)):\n",
    "        HIST[xs[i][0]] = ys[i]\n",
    "    print(\"loaded HIST\")\n",
    "    return HIST\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_train, Y_train, X_eval, Y_eval, model):\n",
    "    \"\"\"Train the KNN model and evaluate it on the validation set.\"\"\"\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred_validation = model.predict(X_eval)\n",
    "    return accuracy_score(Y_eval, y_pred_validation)\n",
    "\n",
    "def gridSearch(X_train, Y_train, X_eval, Y_eval, param_combinations):\n",
    "    #grid search\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    # print(Y_eval)\n",
    "    # print(binary_labels_eval)\n",
    "\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_eval = pca.transform(X_eval)\n",
    "    \n",
    "    # Iterate over each parameter combination\n",
    "    for params in param_combinations:\n",
    "        # Create a new KNN model with the current parameters\n",
    "        model = OneVsRestClassifier(SVC(**params, cache_size=10000))\n",
    "        # Train the model\n",
    "        model.fit(X_train, Y_train)\n",
    "        # Validate the model\n",
    "        y_pred_eval = model.predict(X_eval)\n",
    "    \n",
    "        validation_accuracy = accuracy_score(Y_eval, y_pred_eval)\n",
    "\n",
    "        # Check if this is the best score\n",
    "        if validation_accuracy > best_score:\n",
    "            best_score = validation_accuracy\n",
    "            best_params = params\n",
    "\n",
    "    return best_score, best_params\n",
    "\n",
    "\n",
    "def get_data_dict(X):\n",
    "\n",
    "    print(\"\\nFETCHING IMAGES FROM DIRECTORY\\n\")\n",
    "\n",
    "    image_paths = [f\"train_new_ims/{img[0]}\" for img in X]\n",
    "    img_dict = {}\n",
    "    total_images = len(image_paths)\n",
    "\n",
    "    def progress_callback(future):\n",
    "        \"\"\"Callback function to update progress.\"\"\"\n",
    "        nonlocal loaded_images\n",
    "        loaded_images += 1\n",
    "        percentage = (loaded_images / total_images) * 100\n",
    "        print('\\r\\033[K', end='')\n",
    "        print(f\"\\rProgress: {percentage:.2f}%\", end='')\n",
    "\n",
    "    loaded_images = 0\n",
    "\n",
    "    # Use ThreadPoolExecutor to load images in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to load images\n",
    "        futures = {executor.submit(load_image, path): path for path in image_paths}\n",
    "        \n",
    "        # Attach a callback to each future to update progress\n",
    "        for future in futures:\n",
    "            future.add_done_callback(progress_callback)\n",
    "\n",
    "        # Collect results\n",
    "        for future in as_completed(futures):\n",
    "            img_path = futures[future]\n",
    "            img_dict[img_path.split('/')[-1]] = future.result()  # Store the result in the dictionary\n",
    "        \n",
    "    print(len(list(img_dict.keys())))\n",
    "    return img_dict\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img_ = tf.keras.preprocessing.image.load_img(path)  # Load image\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img_)  # Convert to array\n",
    "    mean = np.mean(img_array)\n",
    "    std_dev = np.std(img_array)\n",
    "    img_array = (img_array - mean) / std_dev\n",
    "    return img_array.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_dict(X_image_paths, all_images):\n",
    "\n",
    "    # print(\"\\nFETCHING IMAGES FROM DICTIONARY\\n\")\n",
    "    # Extract the image keys (filenames) from X_image_paths\n",
    "    img_keys = [img for img in X_image_paths.flatten()]\n",
    "    # Fetch the corresponding pixel data from the dictionary\n",
    "    img_arrays = [all_images[key].flatten() for key in img_keys if key in all_images]\n",
    "    return np.array(img_arrays)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def baselineTest(xs, ys):\n",
    "\n",
    "    # xs = xs[:5000]\n",
    "    # ys = ys[:5000]\n",
    "    xs, ys = mix_aug_data(xs, ys, \"_augmented.jpg\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.1, random_state=42)\n",
    "    all_images = get_data_dict(xs)\n",
    "\n",
    "    # # Example of adjusting PCA components\n",
    "    # pca = PCA(n_components=0.95)  # Try varying this value\n",
    "    # pca.fit(X_train_pixel)\n",
    "    # X_train_pixel = pca.transform(X_train_pixel)\n",
    "    # X_test_pixel = pca.transform(X_test_pixel)\n",
    "\n",
    "    # # Example of using class weights in SVC\n",
    "    # model = OneVsRestClassifier(SVC(C=10, class_weight='balanced'))\n",
    "    # model.fit(X_train_pixel, y_train)\n",
    "    # y_pred = model.predict(X_test_pixel)\n",
    "    # baseline_score = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"Baseline Model Accuracy: {baseline_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # X_train_pixel = get_data_from_dict(X_train, all_images)\n",
    "    # X_test_pixel = get_data_from_dict(X_test, all_images)\n",
    "    all_hogs = load_dict_HOG()\n",
    "    X_train_GS = get_gray_scales(X_train)\n",
    "    X_test_GS = get_gray_scales(X_test)\n",
    "    X_train_HOG = get_data_from_dict(X_train_GS, all_hogs)\n",
    "    X_test_HOG = get_data_from_dict(X_test_GS, all_hogs)\n",
    "    all_hists = load_dict_HIST()\n",
    "    X_train_HIST = get_data_from_dict(X_train, all_hists)\n",
    "    X_test_HIST = get_data_from_dict(X_test, all_hists)\n",
    "\n",
    "    X_train_HH = np.hstack((X_train_HOG, X_train_HIST))\n",
    "    X_test_HH = np.hstack((X_test_HOG, X_test_HIST))\n",
    "\n",
    "    pca = PCA(n_components=0.5)  # Try varying this value\n",
    "    pca.fit(X_train_HH)\n",
    "    X_train_HH = pca.transform(X_train_HH)\n",
    "    X_test_HH = pca.transform(X_test_HH)\n",
    "\n",
    "    model = OneVsRestClassifier(SVC(C=10, class_weight='balanced' ,verbose=True))\n",
    "    model.fit(X_train_HH, y_train)\n",
    "    y_pred_HIST_HOG = model.predict(X_test_HH)\n",
    "    baseline_score = accuracy_score(y_test, y_pred_HIST_HOG)\n",
    "\n",
    "    print(f\"Baseline Model Accuracy: {baseline_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def complexSol(xs,ys):\n",
    "    xs = xs[:1000]\n",
    "    ys = ys[:1000]\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=22)\n",
    "    all_hogs = load_dict_HOG()\n",
    "    all_hists = load_dict_HIST()\n",
    "    all_image_paths, throwAways = mix_aug_data(xs,ys, \"_augmented.jpg\")\n",
    "    print(f\"\\n\\nTOTAL SAMPLES: {len(all_image_paths)}\")\n",
    "    all_images = get_data_dict(all_image_paths)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 0.2, 1, 10, 100],  # Regularization parameter\n",
    "        'kernel': ['rbf'],   # SVM kernel types\n",
    "        'gamma': ['auto']      # Kernel coefficient\n",
    "    }\n",
    "\n",
    "    # Create a list of parameter combinations\n",
    "    param_combinations = [\n",
    "        \n",
    "        {'C': c, 'kernel': k, 'gamma': g}\n",
    "        for c in param_grid['C']\n",
    "        for k in param_grid['kernel']\n",
    "        for g in param_grid['gamma']\n",
    "    ]\n",
    "\n",
    "    total_accuracy = 0\n",
    "    number_of_test_rounds = 0\n",
    "    ######### NESTED K FOLD ##############\n",
    "    ######### TRAINING + TEST FOLDS ######\n",
    "    for i, (train_index, test_index) in enumerate(outer_cv.split(xs,ys)):\n",
    "        #not pixels at this point, just image names.\n",
    "        X_train, X_test = xs[train_index], xs[test_index]\n",
    "        Y_train, Y_test = ys[train_index], ys[test_index]\n",
    "        total_validation_accuracy_pixel=0\n",
    "        total_validation_accuracy_hist_hog=0\n",
    "        number_of_validation_rounds = 0\n",
    "\n",
    "        print(f\"OUTER SPLIT: training[{len(X_train)}]\\ttesting[{len(X_test)}]\\ttotal[{len(X_train) + len(X_test)}]\")\n",
    "\n",
    "        best_across_validation = [{'best_score':None, 'best_params':None}, {'best_score':None, 'best_params':None}]\n",
    "\n",
    "        ######### TRAINING + VALIDATION FOLDS ######\n",
    "        for j, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train,Y_train)):\n",
    "            X_inner_train, X_inner_val = X_train[inner_train_index], X_train[inner_val_index]\n",
    "            y_inner_train, y_inner_val = Y_train[inner_train_index], Y_train[inner_val_index]\n",
    "            print(f\"\\tINNER SPLIT: training[{len(X_inner_train)}]\\tvalidation[{len(X_inner_val)}]\\ttotal[{len(X_inner_train) + len(X_inner_val)}]\")\n",
    "            X_inner_train, y_inner_train = mix_aug_data(X_inner_train, y_inner_train, \"_augmented.jpg\")\n",
    "\n",
    "            #####PIXEL TRAIN#####\n",
    "            X_inner_train_pixel = get_data_from_dict(X_inner_train, all_images)\n",
    "            X_inner_val_pixel = get_data_from_dict(X_inner_val, all_images)\n",
    "\n",
    "            best_score_pixel, best_params_pixel = gridSearch(X_inner_train_pixel, y_inner_train, X_inner_val_pixel, y_inner_val, param_combinations)\n",
    "            print(f\"PIXEL\\tOuter Fold {i + 1}, Inner Fold {j + 1}, Best Accuracy: {best_score_pixel:.4f}, Best params: {best_params_pixel}\")\n",
    "            if (best_across_validation[0]['best_score'] == None or best_score_pixel > best_across_validation[0]['best_score']):\n",
    "                best_across_validation[0]['best_score'] = best_score_pixel\n",
    "                best_across_validation[0]['best_params'] = best_params_pixel\n",
    "\n",
    "\n",
    "            #####HIST HOG TRAIN#####\n",
    "            X_inner_train_hist = get_data_from_dict(X_inner_train, all_hists)\n",
    "            X_inner_val_hist = get_data_from_dict(X_inner_val, all_hists)\n",
    "\n",
    "            X_inner_train_GS = get_gray_scales(X_inner_train)\n",
    "            X_inner_val_GS = get_gray_scales(X_inner_val)\n",
    "            X_inner_train_hog = get_data_from_dict(X_inner_train_GS, all_hogs)\n",
    "            X_inner_val_hog = get_data_from_dict(X_inner_val_GS, all_hogs)\n",
    "\n",
    "            X_inner_train_HH = np.concatenate(X_inner_train_hist, X_inner_train_hog)\n",
    "            X_inner_val_HH = np.concatenate(X_inner_val_hist, X_inner_val_hog)\n",
    "\n",
    "            pca = PCA(n_components=0.90)  # Try varying this value\n",
    "            pca.fit(X_inner_train_HH)\n",
    "            X_inner_train_HH = pca.transform(X_inner_train_HH)\n",
    "            X_inner_val_HH = pca.transform(X_inner_val_HH)\n",
    "\n",
    "            best_score_hist_hog, best_params_hist_hog = gridSearch(X_inner_train_HH, y_inner_train, X_inner_val_HH, y_inner_val, param_combinations)\n",
    "\n",
    "            print(f\"HIST HOG\\tOuter Fold {i + 1}, Inner Fold {j + 1}, Best Accuracy: {best_score_hist_hog:.4f}, Best params: {best_params_hist_hog}\")\n",
    "            if (best_across_validation[1]['best_score'] == None or best_score_hist_hog > best_across_validation[1]['best_score']):\n",
    "                best_across_validation[1]['best_score'] = best_score_hist_hog\n",
    "                best_across_validation[1]['best_params'] = best_params_hist_hog\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # #####HOG TRAIN#####\n",
    "            # X_inner_train_hog = get_data_from_dict(X_inner_train_GS, all_hogs)\n",
    "            # X_inner_val_hog = get_data_from_dict(X_inner_val_GS, all_hogs)\n",
    "            # best_score_hog, best_params_hog = gridSearch(X_inner_train_hog, y_inner_train, X_inner_val_hog, y_inner_val, param_combinations)\n",
    "            # print(f\"HOG\\tOuter Fold {i + 1}, Inner Fold {j + 1}, Best Accuracy: {best_score_hog:.4f}, Best params: {best_params_hog}\")\n",
    "            # if (best_across_validation[2]['best_score'] == None or best_score_hog > best_across_validation[2]['best_score']):\n",
    "            #     best_across_validation[2]['best_score'] = best_score_hog\n",
    "            #     best_across_validation[2]['best_params'] = best_params_hog\n",
    "\n",
    "            total_validation_accuracy_pixel += best_score_pixel\n",
    "            total_validation_accuracy_hist_hog += best_score_hist_hog\n",
    "            # total_validation_accuracy_hog += best_score_hog\n",
    "            number_of_validation_rounds += 1\n",
    "        \n",
    "        average_validation_acc_pixel = total_validation_accuracy_pixel/number_of_validation_rounds\n",
    "        average_validation_acc_hist_hog = total_validation_accuracy_hist_hog/number_of_validation_rounds\n",
    "        # average_validation_acc_hog = total_validation_accuracy_hog/number_of_validation_rounds\n",
    "        print(f\"PIXEL\\tAverage Validation Accuracy: {average_validation_acc_pixel:.4f}\")\n",
    "        print(f\"HIST_HOG\\tAverage Validation Accuracy: {average_validation_acc_hist_hog:.4f}\")\n",
    "        # print(f\"HOG\\tAverage Validation Accuracy: {average_validation_acc_hog:.4f}\")\n",
    "\n",
    "        ensemble = []\n",
    "        for classifier in best_across_validation:\n",
    "            ensemble.append(OneVsRestClassifier(SVC(**classifier['best_params'], cache_size=10000)))\n",
    "\n",
    "        X_train, Y_train = mix_aug_data(X_train, Y_train, \"_augmented.jpg\")\n",
    "        X_train_GS = get_gray_scales(X_train)\n",
    "        X_test_GS = get_gray_scales(Y_train)\n",
    "\n",
    "        #####PIXEL TRAIN#####\n",
    "        X_train_pixel = get_data_from_dict(X_train, all_images)\n",
    "        X_test_pixel = get_data_from_dict(X_test,all_images)\n",
    "        ensemble[0].fit(X_train_pixel, Y_train)\n",
    "        y_pred_pixel = ensemble[0].predict(X_test_pixel)\n",
    "\n",
    "        #####HIST HOG TRAIN ####\n",
    "        X_train_hist = get_data_from_dict(X_train, all_hists)\n",
    "        X_test_hist = get_data_from_dict(X_test,all_hists)\n",
    "        X_train_HOG = get_data_from_dict(X_train_GS, all_hogs)\n",
    "        X_test_HOF = get_data_from_dict(X_test_GS, all_hogs)\n",
    "        X_train_HH = np.concatenate((X_train_hist, X_train_HOG))\n",
    "        X_test_HH = np.concatenate((X_test_hist, X_test_HOF))\n",
    "\n",
    "        \n",
    "        pca = PCA(n_components=0.90)  # Try varying this value\n",
    "        pca.fit(X_train_HH)\n",
    "        X_train_HH = pca.transform(X_train_HH)\n",
    "        X_test_HH = pca.transform(X_test_HH)\n",
    "\n",
    "\n",
    "        ensemble[1].fit(X_train_HH, Y_train)\n",
    "        y_pred_hist_hog = ensemble[0].predict(X_test_HH)\n",
    "\n",
    "\n",
    "        # Assuming y_pred_pixel, y_pred_hist, and y_pred_HOG are your predictions from the models\n",
    "        num_classes = 10  # Adjust this based on your specific use case\n",
    "        num_samples = len(y_pred_pixel)  # Number of samples to predict\n",
    "\n",
    "        # Initialize a vote array\n",
    "        vote_array = np.zeros((num_samples, num_classes), dtype=int)\n",
    "\n",
    "        # Accumulate votes for each prediction\n",
    "        vote_array[np.arange(num_samples), y_pred_pixel] += 1\n",
    "        vote_array[np.arange(num_samples), y_pred_hist_hog] += 1\n",
    "\n",
    "        # Determine final predictions by taking the class with the most votes\n",
    "        final_predictions = np.argmax(vote_array, axis=1)\n",
    "        print(final_predictions[:10])\n",
    "        test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "        print(f\"ENSEMBLE Outer Fold {i+1} Accuracy: {test_accuracy:.4f}\\n\\n\")\n",
    "        total_accuracy += test_accuracy\n",
    "        number_of_test_rounds +=1\n",
    "\n",
    "    average_test_acc = total_accuracy/number_of_test_rounds\n",
    "    print(f\"ENSEMBLE Average Test Accuracy: {average_test_acc:.4f}\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "xs = np.array(df.iloc[:, :-1])\n",
    "ys = np.array(df.iloc[:, -1])\n",
    "\n",
    "baselineTest(xs,ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
